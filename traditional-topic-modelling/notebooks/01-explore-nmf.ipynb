{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e58848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8bd5ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Matrix V: \n",
      " [[1 0 0 1 0 0]\n",
      " [0 1 0 1 1 0]\n",
      " [0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Sample non-negative matrix\n",
    "V = np.array([[1, 0, 0, 1, 0, 0],\n",
    "              [0, 1, 0, 1, 1, 0],\n",
    "              [0, 0, 1, 1, 0, 1]\n",
    "              ])\n",
    "print(\"Orignal Matrix V: \\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02838e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matrix W (Document-Component matrix):\n",
      " [[0.00000000e+00 1.01271248e+00]\n",
      " [1.39363280e+00 2.19581252e-04]\n",
      " [1.39410467e+00 0.00000000e+00]]\n",
      "\n",
      "Matrix H (Component-Term matrix):\n",
      " [[0.00000000e+00 3.58653097e-01 3.58774543e-01 7.17349885e-01\n",
      "  3.58653097e-01 3.58774543e-01]\n",
      " [9.87447056e-01 1.07087784e-04 0.00000000e+00 9.87447115e-01\n",
      "  1.07087784e-04 0.00000000e+00]]\n",
      "Reconstructed V Matrix: \n",
      " [[9.99999953e-01 1.08449135e-04 0.00000000e+00 1.00000001e+00\n",
      "  1.08449135e-04 0.00000000e+00]\n",
      " [2.16824861e-04 4.99830743e-01 4.99999971e-01 9.99939154e-01\n",
      "  4.99830743e-01 4.99999971e-01]\n",
      " [0.00000000e+00 4.99999957e-01 5.00169266e-01 1.00006082e+00\n",
      "  4.99999957e-01 5.00169266e-01]]\n"
     ]
    }
   ],
   "source": [
    "# We set n_components to 2, meaning we want to find a factorization with a rank of 2.\n",
    "# We create an NMF object. \n",
    "# The init='random' parameter tells the algorithm to initialize W and H with random non-negative values. \n",
    "# Random_state=0 is used for reproducibility.\n",
    "\n",
    "n_components = 2\n",
    "model = NMF(n_components=n_components, init='random', random_state=42)\n",
    "W = model.fit_transform(V)\n",
    "H = model.components_\n",
    "\n",
    "print(\"\\nMatrix W (Document-Component matrix):\\n\", W)\n",
    "print(\"\\nMatrix H (Component-Term matrix):\\n\", H)\n",
    "\n",
    "V_constructed = np.dot(W, H)\n",
    "print('Reconstructed V Matrix: \\n', V_constructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382f9ff",
   "metadata": {},
   "source": [
    "**`n_components`**: It determines the number of hidden components (the rank k) that you want to extract from the data. \n",
    "    \n",
    "- **`init`**: This parameter specifies the method used to initialize the matrices W and H before the iterative process begins.\n",
    "    \n",
    "    - `'random'` (default): Initializes W and H with random non-negative numbers.\n",
    "    - `'nndsvd'`: Non-negative Double Singular Value Decomposition. This is a more sophisticated initialization method that often leads to faster convergence and better results than random initialization, especially for data with sparse structure.\n",
    "    - `'nndsvda'`: Similar to `'nndsvd'` but handles cases where the data might have some zero singular values.\n",
    "    - `'nndsvdar'`: Another variant of NNDSVD that incorporates random perturbations.\n",
    "- **`solver`**: This parameter selects the optimization algorithm used to minimize the objective function. Common options include:\n",
    "    \n",
    "    - `'mu'` (default): Multiplicative Update rules, which we discussed earlier. It's generally a safe and widely used option.\n",
    "    - `'cd'`: Coordinate Descent. This is another iterative optimization algorithm that updates one element of W or H at a time. It can sometimes be faster than `'mu'` for certain datasets.\n",
    "- **`beta_loss`**: This parameter is relevant when you want to use a different objective function than the default squared Euclidean distance. It corresponds to the β-divergence.\n",
    "    \n",
    "    - `'frobenius'` (default): This corresponds to the squared Euclidean distance (∣∣V−WH∣∣F2​).\n",
    "    - `'kl'`: This corresponds to the Kullback-Leibler (KL) divergence (generalized to include 0 values in V).\n",
    "    - You can also specify other beta values for different types of divergence, but Frobenius and KL are the most common for NMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be29e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
